---
title: "R Notebook"
output: html_notebook
---

# Intro

This is the primary notebook file within this repo. The notebook imports the libraries, functions, and target
scripts and, afterwards, proceeds to build and run the drake plan for the project. Information regarding the workflow, the functions used, and other useful functions can be found in the reference.Rmd file. 

### Getting the packages and functions required for analysis 
```{r}
source("./setup/libraries.R")       # loads packages, sets paths, and cleans environment 
source(functions)                   # loads functions that we'll be using for analysis
source(targeter)
```


### Build, make, and visualize our drake plan
```{r}

# Running the below code contained within the drake_plan() call simply builds the drake plan itself; it does not 
# run the code. If one thinks of a drake_plan() object as a network graph, running drake_plan() assigns all of the 
# objects to nodes called targets, and it builds edges between each of the targets based on whether each target
# utilizes objects built within another target. Running a make() call on the plan is what actually runs the code
# contained within the plan

plan = drake_plan(
  # importing our raw data
  raw_data= read_csv(district_path),                        
  # getting rid of leaky variables to ensure our model doesn't just select from the various mortality and
  # death-rate-related variables that are heavily correlated with our target 
  mortality_subset = raw_data %>%
    select(contains("Mortality")) %>%           
    select(-YY_Under_Five_Mortality_Rate_U5MR_Total_Person),
  death_subset = raw_data %>%                    
    select(contains("Death")) ,                 
  # assigning the subset of variables related to our target variable to a list which we can then use to filter
  # them out of the raw data
  naughty_list = c(colnames(mortality_subset), colnames(death_subset)),
  non_leaky_data = raw_data %>%
    select(-naughty_list),
  # using an enhanced version of rsample::initial_split() which includes shuffling our rows and setting the split 
  # seed so that we can get a train and test set for our data
  split_data = splitter(non_leaky_data, 0.7, 51),                 
  # using a custom recipes::recipe() function to create a single function that automatically handles all of the 
  # pre-processing of our data (removes NAs, sets our target variable, normalize, etc.). bake() is then used on our   # train and test datasets to pre-process them according to the procedures within our preprocessor() function
  preprocess_recipe = preprocessor(split_data),                
  train_data = bake(preprocess_recipe, training(split_data)),  
  test_data = bake(preprocess_recipe, testing(split_data)),    
  # using our mlrify() function to wrap our pre-processed data in "task" objects that will allow mlr models to be
  # built atop of them. every mlr model requires the data that they use to be input as either a regression 
  # (in the case of this dataset) or a classification task using makeRegrtasK() or makeClassifTask
  mlr_task = target(
    mlrify(data),
    transform = map(data = c(train_data,                  
                             test_data))), 
  # building each of the learners (models) that we'll be using to model our data (our regression task) later on
  # in the pipeline. basic hyperparameters are set for the model so that we can tune them later on 
  learners_rfor = makeLearner("regr.randomForest",
                              par.values = list(
                                ntree = 200,
                                mtry = 4
                              )),
  learners_ela = makeLearner("regr.glmnet",
                               ),
  learners_xgb = makeLearner("regr.xgboost",
                              par.values = list(
                                eta = 0.03,
                                max_depth = 5
                              )),
  learners_svm = makeLearner("regr.svm",
                              par.values = list(
                                kernel = "linear"
                              )),
  learners_glm = makeLearner("regr.glm"
                              ),
  # creating makeParamSet() objects to store the hyperparameters that we will be tuning later. only basic 
  # hyperparameters were chosen at the time of writing this comment. there are no hyperparameters for the glm
  # model, and I was unable to get the elasticnet/glmnet hyperparameters to work properly
  rfor_params = makeParamSet(
    makeIntegerParam("ntree", lower = 100, upper = 800),
    makeIntegerParam("mtry", lower = 2, upper = 16)),
  xgb_params = makeParamSet(
    makeNumericParam("eta", lower = 0.02, upper = 0.2),
    makeIntegerParam("max_depth", lower = 2, upper = 10)),
  svm_params = makeParamSet(
    makeDiscreteParam("kernel", c("linear", "polynomial", "radial", "sigmoid"))),
  # creating a makeTuneControlRandom() object to indicate what method we will be using to tune our hyperparameters.
  # in this example, we will complete 100 iterations of a random hyperparemeter search wherein the most performant
  # set of hyperparameters will be chosen from the 100 iterations
  ctrl_others = makeTuneControlRandom(maxit = 100L),
  # creating a separate "control" object for our glm model because it doesn't have any hyperparameters to tune and
  # because we need to force it to select a subset of our total features to avoid multicollinearity 
  ctrl_glm = makeFeatSelControlSequential(method = "sfs", alpha = 0.02),
  # creating a makeResampleDesc() object to assign our chosen method of model cross-validation, in this case
  # three-fold cross-validation (would have done more folds, however our sample is a bit small)
  resample = makeResampleDesc("CV", iters = 3L),
  # performing stepwise feature selection with three-fold cross-validation on our glm model to create a 
  # "tuned" glm model
  glmfeats = makeFeatSelWrapper(
    learner = learners_glm,
    resampling = resample,
    control = ctrl_glm),
  # selecting mean-squared error (mse) as our loss metric of interest for hyperparameter tuning 
  measure = mse,
  # using the tuneParams() function to tune each model (learner) and their hyperparameters (par.set) via
  # minimizing mse (measure) through three-fold cross-validation (control) using our training dataset (task).
  # again, elasticnet/glmnet was not tuned because of a bug I encountered when setting the hyperparameters
  rfor_tuning = tuneParams(learners_rfor,
                                 task = mlr_task_train_data,
                                 resampling = resample,
                                 measures = measure,
                                 par.set = rfor_params,
                                 control = ctrl_others),
  xgb_tuning = tuneParams(learners_xgb,
                                 task = mlr_task_train_data,
                                 resampling = resample,
                                 measures = measure,
                                 par.set = xgb_params,
                                 control = ctrl_others),
  svm_tuning = tuneParams(learners_svm,
                                 task = mlr_task_train_data,
                                 resampling = resample,
                                 measures = measure,
                                 par.set = svm_params,
                                 control = ctrl_others),
  # using the tuned hyperparameters that we found from tuneParams() to create models (learners) with our 
  # ideal set of tuned hyperparameters 
  rfor_tuned = setHyperPars(learner = learners_rfor,
                            par.values = rfor_tuning$x),
  xgb_tuned = setHyperPars(learner = learners_xgb,
                            par.values = xgb_tuning$x),
  svm_tuned = setHyperPars(learner = learners_svm,
                            par.values = svm_tuning$x),
  # using the tuned models that we just made to put it in a form that can make predictiosn on new data(?)
  rfor_trained = train(rfor_tuned, mlr_task_train_data),
  ela_trained = train(learners_ela, mlr_task_train_data),
  xgb_trained = train(xgb_tuned, mlr_task_train_data),
  svm_trained = train(svm_tuned, mlr_task_train_data),
  glm_trained = train(glmfeats, mlr_task_train_data),
  # using our trained, tuned models to make predictions on our test data (test task). prediction is accomplished
  # via mlr::predict() wrapped in a function (predictor()) that automatically pulls out the data in a dataframe
  # and rounds our predictions and the true values to the specified number of digits
  results_rfor = predictor(rfor_trained, mlr_task_test_data, 3),
  results_ela = predictor(ela_trained, mlr_task_test_data, 3),
  results_xgb = predictor(xgb_trained, mlr_task_test_data, 3),
  results_svm = predictor(svm_trained, mlr_task_test_data, 3),
  results_glm = predictor(glm_trained, mlr_task_test_data, 3),
  # building separate dataframes for performance metrics calculated from our results data using mlr::metrics(), 
  # which we then bind into a single dataframe and plot using our custom plotter() function - see NOTE
  metrics_rfor = metrics(results_rfor, truth, response) %>%
    select(-.estimator) %>% 
    mutate(model = "randomForest") %>%
    select(model, .metric, .estimate) %>%
    spread(.metric, .estimate),
  metrics_ela = metrics(results_ela, truth, response) %>%
    select(-.estimator) %>% 
    mutate(model = "ElasticNet") %>%
    select(model, .metric, .estimate) %>%
    spread(.metric, .estimate),
  metrics_xgb = metrics(results_xgb, truth, response) %>%
    select(-.estimator) %>% 
    mutate(model = "xgBoost") %>%
    select(model, .metric, .estimate) %>%
    spread(.metric, .estimate),
  metrics_svm = metrics(results_svm, truth, response) %>%
    select(-.estimator) %>% 
    mutate(model = "SVM") %>%
    select(model, .metric, .estimate) %>%
    spread(.metric, .estimate),
  metrics_glm = metrics(results_glm, truth, response) %>%
    select(-.estimator) %>% 
    mutate(model = "GLM") %>%
    select(model, .metric, .estimate) %>%
    spread(.metric, .estimate),
  metrics_all = 
    bind_rows(metrics_rfor, metrics_ela, metrics_xgb, metrics_svm, metrics_glm),
  metrics_plot = plotter(metrics_all),
  # building feature importance measures using iml::FeatureImp() wrapped in custom functions that facilitate 
  # translation from mlr, separate procedures for our glm model, and automatic plotting - see NOTE
  importance_rfor = other_featureimport(rfor_trained, train_data),
  importance_ela = other_featureimport(ela_trained, train_data),
  importance_xgb = other_featureimport(xgb_trained, train_data),  # prints out a lot of crap, just ignore it 
  importance_svm = other_featureimport(svm_trained, train_data),
  importance_glm = glm_featureimport(glm_trained, train_data)
)

##### RUN THIS TO EXECUTE CODE #####
# Running make() on our plan object to actually run the code contained within the plan call 
make(plan)
##### RUN THIS TO EXECUTE CODE #####

##### RUN THESE TO VIEW DRAKE NETWORK PLOTS
# Running drake_config() on our plan object to sanitize and reformat the plan parameters into a format that is 
# usable by make() 
config = drake_config(plan)

# Visualizing the state of our drake plan after running make() on it 
vis_drake_graph(config, hover = TRUE)
##### RUN THESE TO VIEW DRAKE NETWORK PLOTS
```
