vis_drake_graph(config, hover = TRUE)
make(plan)
vis_drake_graph(config, hover = TRUE)
source("./setup/libraries.R")       # loads packages, sets paths, and cleans environment
source(functions)                   # loads functions that we'll be using for analysis
source(targeter)
plan = drake_plan(
#
raw_data= read_csv(district_path),                        # import our raw data
# get rid of leaky variables
mortality_subset = raw_data %>%
select(contains("Mortality")) %>%
select(-YY_Under_Five_Mortality_Rate_U5MR_Total_Person),
death_subset = raw_data %>%
select(contains("Death")) ,
naughty_list = c(colnames(mortality_subset), colnames(death_subset)),
non_leaky_data = raw_data %>%
select(-naughty_list),
#
split_data = splitter(non_leaky_data, 0.7, 51),                 # shuffle and split our raw data
#
preprocess_recipe = preprocessor(split_data),                # fix raw data to remove missings, standardize, etc.
train_data = bake(preprocess_recipe, training(split_data)),  # apply preprocessing to training data
test_data = bake(preprocess_recipe, testing(split_data)),    # apply preprocessing to testing data
# save our target variable for later
#
mlr_task = target(
mlrify(data),
transform = map(data = c(train_data,                    # put our preprocessed train/test data into MLR wrapper
test_data))),
#
learners_rfor = makeLearner("regr.randomForest",
par.values = list(
ntree = 200,
mtry = 4
)),
learners_ela = makeLearner("regr.glmnet",
),
learners_xgb = makeLearner("regr.xgboost",
par.values = list(
eta = 0.03,
max_depth = 5
)),
learners_svm = makeLearner("regr.svm",
par.values = list(
kernel = "linear"
)),
learners_glm = makeLearner("regr.glm"
),
#
rfor_params = makeParamSet(
makeIntegerParam("ntree", lower = 100, upper = 800),
makeIntegerParam("mtry", lower = 2, upper = 16)),
# ela_params = makeParamSet(
#   makeDiscreteParam("lambda", seq(0.0, 0.9, 0.1))),
xgb_params = makeParamSet(
makeNumericParam("eta", lower = 0.02, upper = 0.2),
makeIntegerParam("max_depth", lower = 2, upper = 10)),
svm_params = makeParamSet(
makeDiscreteParam("kernel", c("linear", "polynomial", "radial", "sigmoid"))),
#
ctrl_others = makeTuneControlRandom(maxit = 100L),
#
ctrl_glm = makeFeatSelControlSequential(method = "sfs", alpha = 0.02),
#
resample = makeResampleDesc("CV", iters = 3L),
#
glmfeats = makeFeatSelWrapper(
learner = learners_glm,
resampling = resample,
control = ctrl_glm),
#
measure = mse,
#
rfor_tuning = tuneParams(learners_rfor,
task = mlr_task_train_data,
resampling = resample,
measures = measure,
par.set = rfor_params,
control = ctrl_others),
# ela_tuning = tuneParams(learners_ela,
#                                task = mlr_task_train_data,
#                                resampling = resample,
#                                measures = measure,
#                                par.set = ela_params,
#                                control = ctrl_others),
xgb_tuning = tuneParams(learners_xgb,
task = mlr_task_train_data,
resampling = resample,
measures = measure,
par.set = xgb_params,
control = ctrl_others),
svm_tuning = tuneParams(learners_svm,
task = mlr_task_train_data,
resampling = resample,
measures = measure,
par.set = svm_params,
control = ctrl_others),
rfor_tuned = setHyperPars(learner = learners_rfor,
par.values = rfor_tuning$x),
# ela_tuned = setHyperPars(learner = learners_ela,
#                           par.values = ela_tuning$x),
xgb_tuned = setHyperPars(learner = learners_xgb,
par.values = xgb_tuning$x),
svm_tuned = setHyperPars(learner = learners_svm,
par.values = svm_tuning$x),
#
rfor_trained = train(rfor_tuned, mlr_task_train_data),
ela_trained = train(learners_ela, mlr_task_train_data),
xgb_trained = train(xgb_tuned, mlr_task_train_data),
svm_trained = train(svm_tuned, mlr_task_train_data),
glm_trained = train(glmfeats, mlr_task_train_data),
#
results_rfor = predictor(rfor_trained, mlr_task_test_data, 3),
results_ela = predictor(ela_trained, mlr_task_test_data, 3),
results_xgb = predictor(xgb_trained, mlr_task_test_data, 3),
results_svm = predictor(svm_trained, mlr_task_test_data, 3),
results_glm = predictor(glm_trained, mlr_task_test_data, 3),
# want to merge proper id names at this point, but already spent an inordinate amount of time on it...
# building metrics - see NOTE
metrics_rfor = metrics(results_rfor, truth, response) %>%
select(-.estimator) %>%
mutate(model = "randomForest") %>%
select(model, .metric, .estimate) %>%
spread(.metric, .estimate),
metrics_ela = metrics(results_ela, truth, response) %>%
select(-.estimator) %>%
mutate(model = "ElasticNet") %>%
select(model, .metric, .estimate) %>%
spread(.metric, .estimate),
metrics_xgb = metrics(results_xgb, truth, response) %>%
select(-.estimator) %>%
mutate(model = "xgBoost") %>%
select(model, .metric, .estimate) %>%
spread(.metric, .estimate),
metrics_svm = metrics(results_svm, truth, response) %>%
select(-.estimator) %>%
mutate(model = "SVM") %>%
select(model, .metric, .estimate) %>%
spread(.metric, .estimate),
metrics_glm = metrics(results_glm, truth, response) %>%
select(-.estimator) %>%
mutate(model = "GLM") %>%
select(model, .metric, .estimate) %>%
spread(.metric, .estimate),
#
metrics_all =
bind_rows(metrics_rfor, metrics_ela, metrics_xgb, metrics_svm, metrics_glm),
# plotting our metrics: probably want to functionalize this
metrics_plot = plotter(metrics_all),
# interpretability time
importance_rfor = other_featureimport(rfor_trained, train_data),
importance_ela = other_featureimport(ela_trained, train_data),
importance_xgb = other_featureimport(xgb_trained, train_data),  # prints out a lot of shit, just ignore
importance_svm = other_featureimport(svm_trained, train_data),
importance_glm = glm_featureimport(glm_trained, train_data)
)
config = drake_config(plan)
vis_drake_graph(config, hover = TRUE)
make(plan)
View(drake_history())
# glm_params = makeParamSet()  need to figure out stepwise selection in MLR
# makeFeatSelControlSequential  ?????
# results = target(
#   train(learner, task),
#   transform = cross(leaner, task))
### NOTE:
# loadd(train_data)
# glimpse(train_data)
# train_data$LL_Total_Fertility_Rate_Urban
# pulled above code out because I was getting a strange error with the train task due to columns of all NAs. fixed
# by adding a line to recipes to remove variables /w zero variance
### NOTE:
# Still need to figure more about how to use transform() within drake
### NOTE:
# Check what a MLR model's hyperparameters are via getParamSet(makeLearner("regr.glmnet")) or similar for other
# models
### NOTE:
# Drake::make() will sometimes rerun old targets if you add enough new targets that are early in the pipelinel
# still unsure why this occurs
### NOTE:
# Ought to look into using mlr::benchmark() in the future
### NOTE:
# Still need to merge district names onto our results dataframe
### NOTE:
# Still need to functionalize/loop over the yardstick tibbles for cleanliness sake
### NOTE:
# Not sure where elastic net is getting the lambda values that its using prior to tuning
### ERROR:
# Can't get tuned lambda values for glmnet to feed in correctly to setHyperPars
### ERROR:
# Unsure if vis_drake_graph can expand beyond 15 entries per column due to it not being able to show an additional
# learner on top of what we have now
### ERROR:
# Still unable to include id variables in the training datasets when they proceed to modeling. There has to be some
# way to make MLR ignore them!
### ERROR:
# Still need to get stepwise selection working with GLM, because otherwise we have a serious overfitting issue!
### ERROR:
# Function targets aren't showing up as outdated when I update them?
### ERROR:
# We have 85 observations in testing(split_data), so why do we only have 56 in the actual results after predicting?
### BUG:
# In current xgboost version that makes it act all weird when I try to do feature importance for it. pull github
# version when I can
# need to do something about how long and wide this plan is getting, or at least find a way to do good indents
View(drake_history())
plan = drake_plan(
#
raw_data= read_csv(district_path),                        # import our raw data
# get rid of leaky variables
mortality_subset = raw_data %>%
select(contains("Mortality")) %>%
select(-YY_Under_Five_Mortality_Rate_U5MR_Total_Person),
death_subset = raw_data %>%
select(contains("Death")) ,
naughty_list = c(colnames(mortality_subset), colnames(death_subset)),
non_leaky_data = raw_data %>%
select(-naughty_list),
#
split_data = splitter(non_leaky_data, 0.7, 51),                 # shuffle and split our raw data
#
preprocess_recipe = preprocessor(split_data),                # fix raw data to remove missings, standardize, etc.
train_data = bake(preprocess_recipe, training(split_data)),  # apply preprocessing to training data
test_data = bake(preprocess_recipe, testing(split_data)),    # apply preprocessing to testing data
# save our target variable for later
#
mlr_task = target(
mlrify(data),
transform = map(data = c(train_data,                    # put our preprocessed train/test data into MLR wrapper
test_data))),
#
learners_rfor = makeLearner("regr.randomForest",
par.values = list(
ntree = 200,
mtry = 4
)),
learners_ela = makeLearner("regr.glmnet",
),
learners_xgb = makeLearner("regr.xgboost",
par.values = list(
eta = 0.03,
max_depth = 5
)),
learners_svm = makeLearner("regr.svm",
par.values = list(
kernel = "linear"
)),
learners_glm = makeLearner("regr.glm"
),
#
rfor_params = makeParamSet(
makeIntegerParam("ntree", lower = 100, upper = 800),
makeIntegerParam("mtry", lower = 2, upper = 16)),
# ela_params = makeParamSet(
#   makeDiscreteParam("lambda", seq(0.0, 0.9, 0.1))),
xgb_params = makeParamSet(
makeNumericParam("eta", lower = 0.02, upper = 0.2),
makeIntegerParam("max_depth", lower = 2, upper = 10)),
svm_params = makeParamSet(
makeDiscreteParam("kernel", c("linear", "polynomial", "radial", "sigmoid"))),
#
ctrl_others = makeTuneControlRandom(maxit = 100L),
#
ctrl_glm = makeFeatSelControlSequential(method = "sfs", alpha = 0.02),
#
resample = makeResampleDesc("CV", iters = 3L),
#
glmfeats = makeFeatSelWrapper(
learner = learners_glm,
resampling = resample,
control = ctrl_glm),
#
measure = mse,
#
rfor_tuning = tuneParams(learners_rfor,
task = mlr_task_train_data,
resampling = resample,
measures = measure,
par.set = rfor_params,
control = ctrl_others),
# ela_tuning = tuneParams(learners_ela,
#                                task = mlr_task_train_data,
#                                resampling = resample,
#                                measures = measure,
#                                par.set = ela_params,
#                                control = ctrl_others),
xgb_tuning = tuneParams(learners_xgb,
task = mlr_task_train_data,
resampling = resample,
measures = measure,
par.set = xgb_params,
control = ctrl_others),
svm_tuning = tuneParams(learners_svm,
task = mlr_task_train_data,
resampling = resample,
measures = measure,
par.set = svm_params,
control = ctrl_others),
rfor_tuned = setHyperPars(learner = learners_rfor,
par.values = rfor_tuning$x),
# ela_tuned = setHyperPars(learner = learners_ela,
#                           par.values = ela_tuning$x),
xgb_tuned = setHyperPars(learner = learners_xgb,
par.values = xgb_tuning$x),
svm_tuned = setHyperPars(learner = learners_svm,
par.values = svm_tuning$x),
#
rfor_trained = train(rfor_tuned, mlr_task_train_data),
ela_trained = train(learners_ela, mlr_task_train_data),
xgb_trained = train(xgb_tuned, mlr_task_train_data),
svm_trained = train(svm_tuned, mlr_task_train_data),
glm_trained = train(glmfeats, mlr_task_train_data),
#
results_rfor = predictor(rfor_trained, mlr_task_test_data, 3),
results_ela = predictor(ela_trained, mlr_task_test_data, 3),
results_xgb = predictor(xgb_trained, mlr_task_test_data, 3),
results_svm = predictor(svm_trained, mlr_task_test_data, 3),
results_glm = predictor(glm_trained, mlr_task_test_data, 3),
# want to merge proper id names at this point, but already spent an inordinate amount of time on it...
# building metrics - see NOTE
metrics_rfor = metrics(results_rfor, truth, response) %>%
select(-.estimator) %>%
mutate(model = "randomForest") %>%
select(model, .metric, .estimate) %>%
spread(.metric, .estimate),
metrics_ela = metrics(results_ela, truth, response) %>%
select(-.estimator) %>%
mutate(model = "ElasticNet") %>%
select(model, .metric, .estimate) %>%
spread(.metric, .estimate),
metrics_xgb = metrics(results_xgb, truth, response) %>%
select(-.estimator) %>%
mutate(model = "xgBoost") %>%
select(model, .metric, .estimate) %>%
spread(.metric, .estimate),
metrics_svm = metrics(results_svm, truth, response) %>%
select(-.estimator) %>%
mutate(model = "SVM") %>%
select(model, .metric, .estimate) %>%
spread(.metric, .estimate),
metrics_glm = metrics(results_glm, truth, response) %>%
select(-.estimator) %>%
mutate(model = "GLM") %>%
select(model, .metric, .estimate) %>%
spread(.metric, .estimate),
#
metrics_all =
bind_rows(metrics_rfor, metrics_ela, metrics_xgb, metrics_svm, metrics_glm),
# plotting our metrics: probably want to functionalize this
metrics_plot = plotter(metrics_all),
# interpretability time
importance_rfor = other_featureimport(rfor_trained, train_data),
importance_ela = other_featureimport(ela_trained, train_data),
importance_xgb = other_featureimport(xgb_trained, train_data),  # prints out a lot of shit, just ignore
importance_svm = other_featureimport(svm_trained, train_data),
importance_glm = glm_featureimport(glm_trained, train_data)
)
config = drake_config(plan)
vis_drake_graph(config, hover = TRUE)
make(plan)
View(arrange(desc(drake_history())
# glm_params = makeParamSet()  need to figure out stepwise selection in MLR
# makeFeatSelControlSequential  ?????
# results = target(
#   train(learner, task),
#   transform = cross(leaner, task))
### NOTE:
# loadd(train_data)
# glimpse(train_data)
# train_data$LL_Total_Fertility_Rate_Urban
# pulled above code out because I was getting a strange error with the train task due to columns of all NAs. fixed
# by adding a line to recipes to remove variables /w zero variance
### NOTE:
# Still need to figure more about how to use transform() within drake
### NOTE:
# Check what a MLR model's hyperparameters are via getParamSet(makeLearner("regr.glmnet")) or similar for other
# models
### NOTE:
# Drake::make() will sometimes rerun old targets if you add enough new targets that are early in the pipelinel
# still unsure why this occurs
### NOTE:
# Ought to look into using mlr::benchmark() in the future
### NOTE:
# Still need to merge district names onto our results dataframe
### NOTE:
# Still need to functionalize/loop over the yardstick tibbles for cleanliness sake
### NOTE:
# Not sure where elastic net is getting the lambda values that its using prior to tuning
### ERROR:
# Can't get tuned lambda values for glmnet to feed in correctly to setHyperPars
### ERROR:
# Unsure if vis_drake_graph can expand beyond 15 entries per column due to it not being able to show an additional
# learner on top of what we have now
### ERROR:
# Still unable to include id variables in the training datasets when they proceed to modeling. There has to be some
# way to make MLR ignore them!
### ERROR:
# Still need to get stepwise selection working with GLM, because otherwise we have a serious overfitting issue!
### ERROR:
# Function targets aren't showing up as outdated when I update them?
### ERROR:
# We have 85 observations in testing(split_data), so why do we only have 56 in the actual results after predicting?
### BUG:
# In current xgboost version that makes it act all weird when I try to do feature importance for it. pull github
# version when I can
# need to do something about how long and wide this plan is getting, or at least find a way to do good indents
vis_drake_graph(config, hover = TRUE)
?train
?predict
?mlr::predict
#
?drake_config
# Install the required packages
packages = c("drake", "visNetwork", "readr", "dplyr", "recipes", "yardstick", "mlr", "ggplot2", "stringr", "here",
"rsample", "iml", "randomForest", "xgboost", "e1071", "tidyr", "tibble", "devtools", "iml")
install.packages(packages)
library(drake)
library(visNetwork)
library(readr)
library(dplyr)
library(recipes)
library(yardstick)
library(mlr)
library(ggplot2)
library(stringr)
library(here)
library(rsample)
library(iml)
library(randomForest)
library(xgboost)
library(e1071)
library(tidyr)
library(tibble)
library(devtools)
library(iml)
# Save a packrat snapshot
packrat::snapshot()
packrat::status()
# Install the required packages
packages = c("drake", "visNetwork", "readr", "dplyr", "recipes", "yardstick", "mlr", "ggplot2", "stringr", "here",
"rsample", "iml", "randomForest", "xgboost", "e1071", "tidyr", "tibble", "devtools", "iml",
"BH",  "plogr")
install.packages(packages)
# Load the packages
library(drake)
library(visNetwork)
library(readr)
library(dplyr)
library(recipes)
library(yardstick)
library(mlr)
library(ggplot2)
library(stringr)
library(here)
library(rsample)
library(iml)
library(randomForest)
library(xgboost)
library(e1071)
library(tidyr)
library(tibble)
library(devtools)
library(iml)
# Save a packrat snapshot
packrat::snapshot()
library(BH)
library(plogr)
# Save a packrat snapshot
packrat::snapshot()
packrat::status()
packrat::restore()
# Restore packrat packages
packrat::restore()
cache = drake_cache ()
source("./setup/libraries.R")       # loads packages, sets paths, and cleans environment
source(functions)                   # loads functions that we'll be using for analysis
source(targeter)
cache = drake_cache ()
cache$driver$path
# Install the required packages if packrat fails for some reason
packages = c("drake", "knitr", "visNetwork", "readr", "dplyr", "recipes", "yardstick", "mlr", "ggplot2", "stringr", "here",
"rsample", "iml", "randomForest", "xgboost", "e1071", "tidyr", "tibble", "devtools", "iml",
"BH",  "plogr")
install.packages(packages)
# Load the packages
library(drake)
library(knitr)
library(visNetwork)
library(readr)
library(dplyr)
library(recipes)
library(yardstick)
library(mlr)
library(ggplot2)
library(stringr)
library(here)
library(rsample)
library(iml)
library(randomForest)
library(xgboost)
library(e1071)
library(tidyr)
library(tibble)
library(devtools)
library(iml)
library(BH)
library(plogr)
packrat::snapshot()
packrat::status()
# Restore packrat packages
packrat::restore()
